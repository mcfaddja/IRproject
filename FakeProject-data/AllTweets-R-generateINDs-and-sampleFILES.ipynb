{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Indicies and Create Sample files: All Tweets (*R*) \n",
    "\n",
    "This notebook generates indices for use in sampling the data as well as exports datasets from the corpus based on those indices.  The indices are exported as well.\n",
    "\n",
    "## Load the data file\n",
    "\n",
    "> First, we store the file names (*and their locations*) of the files containing fake tweets in the string '**fileName0**' and the string array '**fileName**'[ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName0 = 'datasetsFULLcsv/fakeFollowersCSV/tweets.csv'\n",
    "\n",
    "fileNames = c('datasetsFULLcsv/socialSpambots1csv/tweets.csv', 'datasetsFULLcsv/socialSpambots2csv/tweets.csv', 'datasetsFULLcsv/socialSpambots3csv/tweets.csv', 'datasetsFULLcsv/traditionalSpambots1csv/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the CSV file names previously specified in '**fileName0**' and '**fileNames**'[ ], we can now load the file into the _data.frame_( ) named '**fakeCSV**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”"
     ]
    }
   ],
   "source": [
    "fakeCSV = read.csv(fileName0)\n",
    "fakeTweets <- data.frame(userID = fakeCSV$user_id, id = fakeCSV$id, text = fakeCSV$text)\n",
    "\n",
    "for (filename in fileNames) {\n",
    "    temp0 = read.csv(filename)\n",
    "    #fakeCSV <- rbind(fakeCSV, temp0)\n",
    "    temp <- data.frame(userID = temp0$user_id, id = temp0$id, text = temp0$text)\n",
    "    fakeTweets <- rbind(fakeTweets, temp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We now load the file containing real tweets into the _data.frame_( ) named '**fakeCSV**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in scan(file = file, what = what, sep = sep, quote = quote, dec = dec, :\n",
      "“embedded nul(s) found in input”"
     ]
    }
   ],
   "source": [
    "realCSV = read.csv('datasetsFULLcsv/genuineAccountsCSV/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the '**fakeCSV**' and '**realCSV** _data.frame_( )s, we will create two smaller, simpler *data.frame*( )s named '**fakeTweets**' and '**realTweets**', respectively.  This reduction in size and complexity of '**fakeTweets**' is due to the fact that it only contains the ID number of the tweet in our database, along with the text of the tweet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweets <- data.frame(id = fakeCSV$id, text = fakeCSV$text)\n",
    "realTweets <- data.frame(id = realCSV$id, text = realCSV$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The initial size of the imports are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(fakeTweets)\n",
    "nrow(realTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we remove web URLS, twitter usernames, twitter hashtags, punctuation, and stand-alone numeric digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove web URLs\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"http[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"http[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove twitter handles (@<username>)\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"#[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"#[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove hashtags (#<hashtag name>)\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"@[[:alnum:][:punct:]]*\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"@[[:alnum:][:punct:]]*\", \"\", realTweets$text))\n",
    "\n",
    "# remove punctuation\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub('[[:punct:] ]+', ' ', fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub('[[:punct:] ]+', ' ', realTweets$text))\n",
    "\n",
    "# remove numbers\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = gsub(\"[0-9]\", \"\", fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = gsub(\"[0-9]\", \"\", realTweets$text))\n",
    "\n",
    "# convert to lowercase\n",
    "fakeTweets <- data.frame(id = fakeTweets$id, text = tolower(fakeTweets$text))\n",
    "realTweets <- data.frame(id = realTweets$id, text = tolower(realTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of Tweets available after \"cleaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(fakeTweets)\n",
    "nrow(realTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TidyText the data file\n",
    "\n",
    "> Now we must tokenize the text of each tweet using the '*tidytext*' and '*dplyr*' libraries.  First, we must import the '*tidytext*' and '*dplyr*' libraries,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(tidytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we convert the data frames of '**fakeTweets**' and '**realTweets**' to the data frame type from the '*dplyr*' library,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fakeTweets <- data_frame(userID = fakeTweets$userID, id = fakeTweets$id, text = as.character(fakeTweets$text))\n",
    "realTweets <- data_frame(userID = realTweets$userID, id = realTweets$id, text = as.character(realTweets$text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> so that we can finally tokenize the text from each of the tweets,\n",
    "\n",
    "> ### Tokenization\n",
    "\n",
    ">> We now tokenize the text of the fake tweets, storing them in the new data frame (*from __dplyr__*) '**fakeTweetTOKENS**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeTweetsTOKENS <- fakeTweets %>%\n",
    "    unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Similarly, we tokenize the text of the real tweets, them in the new data frame (*from __dplyr__*) '**realTweetTOKENS**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "realTweetsTOKENS <- realTweets %>%\n",
    "    unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1898286"
      ],
      "text/latex": [
       "1898286"
      ],
      "text/markdown": [
       "1898286"
      ],
      "text/plain": [
       "[1] 1898286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "28450570"
      ],
      "text/latex": [
       "28450570"
      ],
      "text/markdown": [
       "28450570"
      ],
      "text/plain": [
       "[1] 28450570"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nrow(fakeTweetsTOKENS)\n",
    "#nrow(realTweetsTOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove '*Stop Words*'\n",
    "\n",
    "> Now, we will remove any stop words from the text of the tweets.  To do this, we first import the '*stop_words*' dataset from the '*tidytext*' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we use the '*anti_join*( )' function from the '*dplyr*' library to remove these stop wrods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n"
     ]
    }
   ],
   "source": [
    "fakeTweetsTOKENS <- fakeTweetsTOKENS %>%\n",
    "    anti_join(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n"
     ]
    }
   ],
   "source": [
    "realTweetsTOKENS <- realTweetsTOKENS %>%\n",
    "    anti_join(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1320088"
      ],
      "text/latex": [
       "1320088"
      ],
      "text/markdown": [
       "1320088"
      ],
      "text/plain": [
       "[1] 1320088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "13654172"
      ],
      "text/latex": [
       "13654172"
      ],
      "text/markdown": [
       "13654172"
      ],
      "text/plain": [
       "[1] 13654172"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(fakeTweetsTOKENS)\n",
    "nrow(realTweetsTOKENS)\n",
    "#realTweetsTOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrcWORDS <- get_sentiments(\"nrc\")\n",
    "nrcEMOTIONS <- unique(nrcWORDS$sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n"
     ]
    }
   ],
   "source": [
    "fakeTweetsNRCsentiment <- data.frame(id = 0)\n",
    "for (emotion in nrcEMOTIONS){\n",
    "    fakeTweetsNRCsentiment0 <- inner_join(fakeTweetsTOKENS, filter(nrcWORDS, sentiment == emotion))\n",
    "    fakeTweetsNRCsentiment <- full_join(fakeTweetsNRCsentiment, fakeTweetsNRCsentiment0)\n",
    "    }\n",
    "fakeTweetsNRCsentiment <- data.frame(fakeTweetsNRCsentiment[-1,])\n",
    "#fakeTweetsNRCsentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n",
      "Joining, by = \"id\"\n",
      "Warning message:\n",
      "“Column `id` joining factors with different levels, coercing to character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”Joining, by = \"word\"\n",
      "Joining, by = c(\"id\", \"userID\", \"word\", \"sentiment\")\n",
      "Warning message:\n",
      "“Column `id` joining character vector and factor, coercing into character vector”"
     ]
    }
   ],
   "source": [
    "realTweetsNRCsentiment <- data.frame(id = as.factor(0))\n",
    "for (emotion in nrcEMOTIONS){\n",
    "    realTweetsNRCsentiment0 <- inner_join(realTweetsTOKENS, filter(nrcWORDS, sentiment == emotion))\n",
    "    realTweetsNRCsentiment <- full_join(realTweetsNRCsentiment, realTweetsNRCsentiment0)\n",
    "    }\n",
    "realTweetsNRCsentiment <- data.frame(realTweetsNRCsentiment[-1,])\n",
    "#realTweetsNRCsentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "245770"
      ],
      "text/latex": [
       "245770"
      ],
      "text/markdown": [
       "245770"
      ],
      "text/plain": [
       "[1] 245770"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6216860"
      ],
      "text/latex": [
       "6216860"
      ],
      "text/markdown": [
       "6216860"
      ],
      "text/plain": [
       "[1] 6216860"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(fakeTweetsNRCsentiment)\n",
    "nrow(realTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attach(fakeTweetsNRCsentiment)\n",
    "fakeNRCscoredTweets <- data.frame(table(id, sentiment), realFAKEcat = \"fake\")\n",
    "detach(fakeTweetsNRCsentiment)\n",
    "\n",
    "attach(realTweetsNRCsentiment)\n",
    "realNRCscoredTweets <- data.frame(table(id, sentiment), realFAKEcat = \"real\")\n",
    "detach(realTweetsNRCsentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NRCscoredTweets <- full_join(fakeNRCscoredTweets, realNRCscoredTweets)\n",
    "NRCscoredTweets <- rbind(fakeNRCscoredTweets, realNRCscoredTweets)\n",
    "#NRCscoredTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "15456890"
      ],
      "text/latex": [
       "15456890"
      ],
      "text/markdown": [
       "15456890"
      ],
      "text/plain": [
       "[1] 15456890"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "245770"
      ],
      "text/latex": [
       "245770"
      ],
      "text/markdown": [
       "245770"
      ],
      "text/plain": [
       "[1] 245770"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6216860"
      ],
      "text/latex": [
       "6216860"
      ],
      "text/markdown": [
       "6216860"
      ],
      "text/plain": [
       "[1] 6216860"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(NRCscoredTweets)\n",
    "nrow(fakeTweetsNRCsentiment)\n",
    "nrow(realTweetsNRCsentiment)\n",
    "\n",
    "fakeTestTrainIND <- sample(1:nrow(fakeNRCscoredTweets), 25000)\n",
    "realTestTrainIND <- sample(1:nrow(realNRCscoredTweets), 1500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'trust'</li>\n",
       "\t<li>'fear'</li>\n",
       "\t<li>'negative'</li>\n",
       "\t<li>'sadness'</li>\n",
       "\t<li>'anger'</li>\n",
       "\t<li>'surprise'</li>\n",
       "\t<li>'positive'</li>\n",
       "\t<li>'disgust'</li>\n",
       "\t<li>'joy'</li>\n",
       "\t<li>'anticipation'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'trust'\n",
       "\\item 'fear'\n",
       "\\item 'negative'\n",
       "\\item 'sadness'\n",
       "\\item 'anger'\n",
       "\\item 'surprise'\n",
       "\\item 'positive'\n",
       "\\item 'disgust'\n",
       "\\item 'joy'\n",
       "\\item 'anticipation'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'trust'\n",
       "2. 'fear'\n",
       "3. 'negative'\n",
       "4. 'sadness'\n",
       "5. 'anger'\n",
       "6. 'surprise'\n",
       "7. 'positive'\n",
       "8. 'disgust'\n",
       "9. 'joy'\n",
       "10. 'anticipation'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"trust\"        \"fear\"         \"negative\"     \"sadness\"      \"anger\"       \n",
       " [6] \"surprise\"     \"positive\"     \"disgust\"      \"joy\"          \"anticipation\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fakeNRCscoredTweets\n",
    "nrcEMOTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scoredFakeTweets <- data.frame()\n",
    "#fakeNRCscoredTweets\n",
    "\n",
    "#NRCscoredTweets\n",
    "#filter(NRCscoredTweets, sentiment == \"joy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trustScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"trust\")$id, trust = filter(NRCscoredTweets, sentiment == \"trust\")$Freq)\n",
    "fearScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"fear\")$id, fear = filter(NRCscoredTweets, sentiment == \"fear\")$Freq)\n",
    "negScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"negative\")$id, negative = filter(NRCscoredTweets, sentiment == \"negative\")$Freq)\n",
    "sadnessScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"sadness\")$id, sadness = filter(NRCscoredTweets, sentiment == \"sadness\")$Freq)\n",
    "angerScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"anger\")$id, anger = filter(NRCscoredTweets, sentiment == \"anger\")$Freq)\n",
    "surpriseScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"surprise\")$id, surprise = filter(NRCscoredTweets, sentiment == \"surprise\")$Freq)\n",
    "posScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"positive\")$id, positive = filter(NRCscoredTweets, sentiment == \"positive\")$Freq)\n",
    "disgustScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"disgust\")$id, disgust = filter(NRCscoredTweets, sentiment == \"disgust\")$Freq)\n",
    "joyScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"joy\")$id, joy = filter(NRCscoredTweets, sentiment == \"joy\")$Freq)\n",
    "anticipationScores <- data.frame(id = filter(NRCscoredTweets, sentiment == \"anticipation\")$id, anticipation = filter(NRCscoredTweets, sentiment == \"anticipation\")$Freq, realFAKEcat = filter(NRCscoredTweets, sentiment == \"anticipation\")$realFAKEcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n",
      "Joining, by = \"id\"\n"
     ]
    }
   ],
   "source": [
    "nrcSCORES <- full_join(trustScores, full_join(fearScores, full_join(negScores, full_join(sadnessScores, full_join(angerScores, full_join(surpriseScores, full_join(posScores, full_join(disgustScores, full_join(joyScores, anticipationScores)))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeNRCscores <- filter(nrcSCORES, realFAKEcat == 'fake')\n",
    "realNRCscores <- filter(nrcSCORES, realFAKEcat == 'real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1545689"
      ],
      "text/latex": [
       "1545689"
      ],
      "text/markdown": [
       "1545689"
      ],
      "text/plain": [
       "[1] 1545689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "58661"
      ],
      "text/latex": [
       "58661"
      ],
      "text/markdown": [
       "58661"
      ],
      "text/plain": [
       "[1] 58661"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1487028"
      ],
      "text/latex": [
       "1487028"
      ],
      "text/markdown": [
       "1487028"
      ],
      "text/plain": [
       "[1] 1487028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(nrcSCORES)\n",
    "nrow(fakeNRCscores)\n",
    "nrow(realNRCscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(158)\n",
    "fakeTRAINind <- sample(1:nrow(fakeNRCscores), 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(231)\n",
    "realTRAINind <- sample(1:nrow(realNRCscores), 1300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "50000"
      ],
      "text/latex": [
       "50000"
      ],
      "text/markdown": [
       "50000"
      ],
      "text/plain": [
       "[1] 50000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "8661"
      ],
      "text/latex": [
       "8661"
      ],
      "text/markdown": [
       "8661"
      ],
      "text/plain": [
       "[1] 8661"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fakeNRCscoresTRAIN <- fakeNRCscores[fakeTRAINind, ]\n",
    "fakeNRCscoresTEST <- fakeNRCscores[-fakeTRAINind, ]\n",
    "nrow(fakeNRCscoresTRAIN)\n",
    "nrow(fakeNRCscoresTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1300000"
      ],
      "text/latex": [
       "1300000"
      ],
      "text/markdown": [
       "1300000"
      ],
      "text/plain": [
       "[1] 1300000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "187028"
      ],
      "text/latex": [
       "187028"
      ],
      "text/markdown": [
       "187028"
      ],
      "text/plain": [
       "[1] 187028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "realNRCscoresTRAIN <- realNRCscores[realTRAINind, ]\n",
    "realNRCscoresTEST <- realNRCscores[-realTRAINind, ]\n",
    "nrow(realNRCscoresTRAIN)\n",
    "nrow(realNRCscoresTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NRCscoresTRAIN <- rbind(fakeNRCscoresTRAIN, realNRCscoresTRAIN)\n",
    "NRCscoresTEST <- rbind(fakeNRCscoresTEST, realNRCscoresTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1350000"
      ],
      "text/latex": [
       "1350000"
      ],
      "text/markdown": [
       "1350000"
      ],
      "text/plain": [
       "[1] 1350000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "195689"
      ],
      "text/latex": [
       "195689"
      ],
      "text/markdown": [
       "195689"
      ],
      "text/plain": [
       "[1] 195689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(NRCscoresTRAIN)\n",
    "nrow(NRCscoresTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(subset(NRCscoresTRAIN, select = -c(id, realFAKEcat)), :\n",
      "“The response has five or fewer unique values.  Are you sure you want to do regression?”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in randomForest.default(subset(NRCscoresTRAIN, select = -c(id, realFAKEcat)), : length of response must be the same as predictors\n",
     "output_type": "error",
     "traceback": [
      "Error in randomForest.default(subset(NRCscoresTRAIN, select = -c(id, realFAKEcat)), : length of response must be the same as predictors\nTraceback:\n",
      "1. randomForest(subset(NRCscoresTRAIN, select = -c(id, realFAKEcat)), \n .     subset(NRCscoresTRAIN, select = c(realFAKEcat)), ntres = 1000)",
      "2. randomForest.default(subset(NRCscoresTRAIN, select = -c(id, realFAKEcat)), \n .     subset(NRCscoresTRAIN, select = c(realFAKEcat)), ntres = 1000)",
      "3. stop(\"length of response must be the same as predictors\")"
     ]
    }
   ],
   "source": [
    "nrcSCORES.rf <- randomForest(subset(NRCscoresTRAIN, select = -c(id,realFAKEcat)), \n",
    "                              subset(NRCscoresTRAIN, select = c(realFAKEcat)),\n",
    "                              subset(NRCscoresTEST, select = -c(id,realFAKEcat)),\n",
    "                              subset(NRCscoresTEST, select = c(realFAKEcat)),\n",
    "                              ntres = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in .subset(x, j): invalid subscript type 'language'\n",
     "output_type": "error",
     "traceback": [
      "Error in .subset(x, j): invalid subscript type 'language'\nTraceback:\n",
      "1. realNRCscoresTEST[, ~c(\"id\", \"realFAKEcat\")]",
      "2. `[.data.frame`(realNRCscoresTEST, , ~c(\"id\", \"realFAKEcat\"))"
     ]
    }
   ],
   "source": [
    "#realNRCscoresTEST[ ,  c(\"id\",\"realFAKEcat\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##df <- subset(df, select = -c(a,c) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#subset(realNRCscoresTEST, select = -c(id,realFAKEcat))\n",
    "\n",
    "NRCtrainIN <- subset(NRCscoresTRAIN, select = -c(id,realFAKEcat))\n",
    "NRCtrainOUT <- subset(NRCscoresTRAIN, select = c(realFAKEcat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(NRCtrainIN, NRCtrainOUT):\n",
      "“The response has five or fewer unique values.  Are you sure you want to do regression?”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in randomForest.default(NRCtrainIN, NRCtrainOUT): length of response must be the same as predictors\n",
     "output_type": "error",
     "traceback": [
      "Error in randomForest.default(NRCtrainIN, NRCtrainOUT): length of response must be the same as predictors\nTraceback:\n",
      "1. randomForest(NRCtrainIN, NRCtrainOUT)",
      "2. randomForest.default(NRCtrainIN, NRCtrainOUT)",
      "3. stop(\"length of response must be the same as predictors\")"
     ]
    }
   ],
   "source": [
    "nrc.rf <- randomForest(NRCtrainIN, NRCtrainOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
